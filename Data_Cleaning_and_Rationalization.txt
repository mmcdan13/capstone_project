Data Cleaning and Rationalization:

Base Datasets - 
- Chipotle_stores.csv 
- 2020_crime.csv 

Initial cleaning of data completed in Jupyter Notebook with python, pandas, numpy, and os.
    2020crime.csv data is for the Chicago area only 
        - 110950 Rows of data with 23 columns including identifiers such as case #, Date/Time, Block(address), Primary Crime Type, Descripton, Location description, Arrest (T/F), Domestic (True/False), Beat, District, Ward, Community, FBI Code, X/Y coordinates, Last updated, Lattitude/Longitude, and Location(geocoordinates)
        - 751 rows did not have lat/long or other locational data.  Initially these rows were dropped.
    
    Chipotle_stores.csv covers stores in the united states from Kagle
        - Dataset has 2629 rows of data and 5 columns including State, City, address with zip, lattitude and longitude for each store.
        - As data came from Kaggle precleaned, initially no initial scrubbing was conducted.

Next steps - 
    Collecting more data - 
        - After team review and discussion with Educational team focus changed to develop model based on Chicago data then validate with other stores in other states/cities.
        - Data gaps identified
            - Recommendation was made to team to add population data to into the analysis for safety.  
                - Best option identified was population by zip within Chicago from same source as crime data.  Chicago_Population_by_Zipcode_2020
                    - Data set has zip, city, population, people/sq mile and national ranking for population.       
            - In order to link to Chipotle data, Zip will need split from address column. And Crime Data will need to have zip added in.
            - Ranking system needed to assess crime severity including identifier of "Bad/Good or Low/High Risk areas".
            - Crime data does not have zip.  Need zip files.  chicago_zip_ward was also found on same chicago data portal.
                - Data set has zip and ward only.  61 total wards included.
    
 2nd Cleaning - 
    Key Takaways -
        - Ranking system - 
            - Chicago has 5 main classes of Felonies (Class X - Class 4) and 3 main classes of misdemeanors(A-C or 1-3 depending upon source). 
                - This lends to an 8 point ranking system with the highest severity crimes at 8 and lowest as 1.
                - Classification of the crimes was manual as multiple websites had to be researched and a consolidated listing was not identified. As such ranking was added in base .csv via excel then updated .csv reloaded to branch
        -Other Data Activities -
            -  Merged datasets for crime and zip based on ward in Crime data to link zip.
            - Pulled zip out to a separate column
                  
            - 
        - Cleaning Review - 
            - Average severity by zipcode is 3.63 to 4.18.
            - Consolidating on zipcode reduces the primary dataset to 43 lines/zipcodes.
            
3rd Cleaning - post first model Runs
		- Added rows for all population information; Arrest & Domestic status, date, ward back from crime data to run through model.
		- Provided updated joined database from python and pandas for modeling.


4th Cleaning - After changing model to neural network and encountering issues with PGAdmin access to send cleaned data for final joins.
	Identified issues with overfit and impact with ranking system.  Solutions identified included:
		- convert Arrest, Domestic, and Chipotle in Zip data to binary once joined.
		- Change join configuration - 
			- First join Zip Ward data to Crime
			- second join Chipotle data to Crime with Zip.
			- final join in PGAdmin to AWS then S3 to feed model.
		-  Data changes
			- For zips with more than 1 chipotle store, filter data to only keep 1 line per zip with a store.  (stops us from generating 17,000 additional crime line items by creating duplicates to support each store address.
			- Once linked to Crime Data convert column to binary 0/1 for no/yes there is a store in that zip code.
			- Drop locational data - Address, Lat, and lng data.  Not needed for model.
			- Convert Arrest and Domestic to binary
			- Add Safety index - Binary 1-3 ranking safe, 4-8 unsafe. Only 13 line items show in model outputs. Still resolving if Rank will stay or go.
			- After final join in python and resolving excessive lines, end result was 18 lines missing.  After pivoting in excel multiple ways, identified that of all the code, only 18 incidents had multiple line items in the original database had multiple arrests. 
			    those 18 lines were in the homicide category.  Those were left in the data set as they are in the highest rank category.
	Final files loaded into pgadmin from the dataframes includes:
		- Individual structure:  
			Server name: Capstone_new/Capstone2 - can vary by individual on team
			Database - postgres
			Tables Loaded - 7
				Original .csv's from internet
					- 2020_Crime, Chicago_Population_by_Zipcode_2020, chicage_zip_ward, and chipotle_data
				New base tables
					- Clean Pop - Cleaned base pop data to pertinent columns only
					- crime_chip_final - Crime cleaned with zip and chip with all binary columns
				
        	
     