{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0950749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "file_to_load = os.path.join('Resources','new_chipotle_data.csv')\n",
    "\n",
    "df = pd.read_csv(ratings_load)\n",
    "kaggle_metadata = pd.read_csv(meta_load, low_memory=False)\n",
    "\n",
    "# Open the read the Wikipedia data JSON file.\n",
    "with open(wiki_load, mode='r') as file:\n",
    "    wiki_movies_raw = json.load(file)\n",
    "\n",
    "# Write a list comprehension to filter out TV shows.\n",
    "wiki_movies = [movie for movie in wiki_movies_raw \n",
    "    if ('Director' in movie or 'Directed by' in movie)\n",
    "        and ('imdb_link' in movie)\n",
    "        and 'No. of episodes' not in movie]\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "\n",
    "# Create db engine\n",
    "engine = create_engine(db_string)\n",
    "\n",
    "# Save dataframe to sql\n",
    "movies_df.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "\n",
    "# Remove ratings table from db with code\n",
    "# Opening a connection\n",
    "connection = engine.raw_connection()\n",
    "\n",
    "# Creating a cursor object using the cursor() method\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Drop ratings table if exists\n",
    "cursor.execute(\"DROP TABLE IF EXISTS ratings\")\n",
    "\n",
    "# Commit your changes\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "# Create the path to your file directory and variables for the file\n",
    "file_dir = r'C:\\Users\\Gurr1\\OneDrive\\Documents\\Bootcamp\\capstone_project\\Resources'\n",
    "\n",
    "# Import rating data to sql using chunksize param\n",
    "# create a variable for the number of rows imported\n",
    "rows_imported = 0\n",
    "\n",
    "# Create time trackign ability\n",
    "start_time = time.time()\n",
    "for data in pd.read_csv(ratings_load):\n",
    "\n",
    "    # print out the range of rows that are being imported\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "    # increment the number of rows imported by the chunk\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    # print that rows have finished importing\n",
    "    # add elapsed time\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
